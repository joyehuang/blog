---
title: ä¸ºä»€ä¹ˆTransformeréœ€è¦å½’ä¸€åŒ–ï¼Ÿä»æ¢¯åº¦æ¶ˆå¤±åˆ°RMSNorm
description: 'æ·±å…¥æ¢è®¨ä¸ºä»€ä¹ˆæ·±å±‚ç¥ç»ç½‘ç»œéœ€è¦å½’ä¸€åŒ–ï¼Œä»¥åŠ RMSNorm å¦‚ä½•æˆä¸ºç°ä»£ LLM çš„æ ‡é…'
publishDate: 2025-12-16 00:00:00
tags: ['LLM', 'Transformer', 'MiniMind', 'Deep Learning', 'Normalization']
---

> æœ¬æ–‡æ˜¯ MiniMind å­¦ä¹ ç³»åˆ—çš„ç¬¬1ç¯‡ï¼Œæ·±å…¥æ¢è®¨ä¸ºä»€ä¹ˆæ·±å±‚ç¥ç»ç½‘ç»œéœ€è¦å½’ä¸€åŒ–ï¼Œä»¥åŠ RMSNorm å¦‚ä½•æˆä¸ºç°ä»£ LLM çš„æ ‡é…ã€‚

## å…³äºæœ¬ç³»åˆ—

[MiniMind](https://github.com/jingyaogong/minimind) æ˜¯ä¸€ä¸ªç®€æ´ä½†å®Œæ•´çš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒé¡¹ç›®ï¼ŒåŒ…å«ä»æ•°æ®å¤„ç†ã€æ¨¡å‹è®­ç»ƒåˆ°æ¨ç†éƒ¨ç½²çš„å®Œæ•´æµç¨‹ã€‚æˆ‘åœ¨å­¦ä¹ è¿™ä¸ªé¡¹ç›®çš„è¿‡ç¨‹ä¸­ï¼Œå°†æ ¸å¿ƒæŠ€æœ¯ç‚¹æ•´ç†æˆäº† [minimind-notes](https://github.com/joyehuang/minimind-notes) ä»“åº“ï¼Œå¹¶äº§å‡ºäº†è¿™ä¸ª4ç¯‡ç³»åˆ—åšå®¢ï¼Œç³»ç»Ÿæ€§åœ°è®²è§£ Transformer çš„æ ¸å¿ƒç»„ä»¶ã€‚

æœ¬ç³»åˆ—åŒ…æ‹¬ï¼š
1. **å½’ä¸€åŒ–æœºåˆ¶**ï¼ˆæœ¬ç¯‡ï¼‰- ä¸ºä»€ä¹ˆéœ€è¦RMSNorm
2. **RoPEä½ç½®ç¼–ç ** - å¦‚ä½•è®©æ¨¡å‹ç†è§£è¯åº
3. **Attentionæœºåˆ¶** - Transformerçš„æ ¸å¿ƒå¼•æ“
4. **FeedForwardä¸å®Œæ•´æ¶æ„** - ç»„ä»¶å¦‚ä½•ååŒå·¥ä½œ

---

## ä¸€ã€å¼•è¨€

### 1.1 ä¸€ä¸ªå¸¸è§çš„ç–‘é—®

å¦‚æœä½ æ‰“å¼€ Transformer çš„ä»£ç ï¼Œä¼šå‘ç°åˆ°å¤„éƒ½æ˜¯ Normalization å±‚ï¼š

```python
class TransformerBlock(nn.Module):
    def forward(self, x):
        x = self.input_norm(x)        # â† Normå±‚
        x = self.attention(x)

        x = self.post_attn_norm(x)    # â† åˆæ˜¯Normå±‚
        x = self.feedforward(x)
        return x
```

**ç–‘é—®**ï¼š
- ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¹ˆå¤šå½’ä¸€åŒ–ï¼Ÿ
- å»æ‰ä¸è¡Œå—ï¼Ÿ
- RMSNorm å’Œ LayerNorm æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

### 1.2 æœ¬æ–‡è¦å›ç­”çš„é—®é¢˜

- æ·±å±‚ç½‘ç»œä¸ºä»€ä¹ˆä¼š**æ¢¯åº¦æ¶ˆå¤±**ï¼Ÿï¼ˆä¸æ˜¯ç„å­¦ï¼Œæœ‰æ•°å­¦è¯æ˜ï¼‰
- RMSNorm åšäº†ä»€ä¹ˆï¼Ÿï¼ˆä¸æ˜¯ä¸¢å¤±ä¿¡æ¯ï¼‰
- ä¸ºä»€ä¹ˆç°ä»£ LLM éƒ½ä» LayerNorm è¿ç§»åˆ° RMSNormï¼Ÿ
- RMSNorm åœ¨ Transformer ä¸­çš„å…·ä½“ä½ç½®åœ¨å“ªï¼Ÿ

---

## äºŒã€æ¢¯åº¦æ¶ˆå¤±çš„çœŸç›¸

### 2.1 é—®é¢˜æ¼”ç¤ºï¼š8å±‚ç½‘ç»œçš„ç¾éš¾

è®©æˆ‘ä»¬ç”¨ä»£ç æ¼”ç¤ºä¸€ä¸ªæ²¡æœ‰å½’ä¸€åŒ–çš„8å±‚ç½‘ç»œä¼šå‘ç”Ÿä»€ä¹ˆï¼š

```python
import torch
import torch.nn as nn

# ä¸€ä¸ªæ²¡æœ‰å½’ä¸€åŒ–çš„8å±‚ç½‘ç»œ
class DeepNetworkWithoutNorm(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Linear(512, 512) for _ in range(8)
        ])

    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = layer(x)
            x = torch.relu(x)
            print(f"ç¬¬{i+1}å±‚æ ‡å‡†å·®: {x.std().item():.4f}")
        return x

# æµ‹è¯•
model = DeepNetworkWithoutNorm()
x = torch.randn(32, 512)
print(f"è¾“å…¥æ ‡å‡†å·®: {x.std().item():.4f}")

output = model(x)
```

**è¾“å‡ºç»“æœ**ï¼š
```
è¾“å…¥æ ‡å‡†å·®: 1.0405
ç¬¬1å±‚æ ‡å‡†å·®: 0.8932
ç¬¬2å±‚æ ‡å‡†å·®: 0.6421
ç¬¬3å±‚æ ‡å‡†å·®: 0.4156
ç¬¬4å±‚æ ‡å‡†å·®: 0.2387
ç¬¬5å±‚æ ‡å‡†å·®: 0.1024
ç¬¬6å±‚æ ‡å‡†å·®: 0.0432
ç¬¬7å±‚æ ‡å‡†å·®: 0.0198
ç¬¬8å±‚æ ‡å‡†å·®: 0.0163  â† å‡ ä¹å½’é›¶ï¼
```

### 2.2 ä¸ºä»€ä¹ˆä¼šè¶Šæ¥è¶Šå°ï¼Ÿ

**æ•°å­¦è§£é‡Š**ï¼š

1. **æ¯å±‚çš„çŸ©é˜µä¹˜æ³•**ï¼š`y = Wx`
   - å¦‚æœæƒé‡ W åˆå§‹åŒ–ä¸ºå‡å€¼0ã€æ ‡å‡†å·®1çš„æ­£æ€åˆ†å¸ƒ
   - è¾“å‡ºçš„æ ‡å‡†å·®çº¦ç­‰äºï¼š`std(y) â‰ˆ std(x) Ã— sqrt(input_dim) / sqrt(output_dim)`

2. **ReLU æ¿€æ´»å‡½æ•°çš„å½±å“**ï¼š
   - ReLU(x) = max(0, x)
   - è´Ÿæ•°å…¨éƒ¨å˜æˆ0
   - è¿›ä¸€æ­¥å‡å°æ ‡å‡†å·®ï¼ˆçº¦å‡åŠï¼‰

3. **å¤šå±‚ç´¯ç§¯æ•ˆåº”**ï¼š
   - æ¯å±‚æ ‡å‡†å·® Ã— kï¼Œå…¶ä¸­ k < 1
   - 8å±‚åï¼š`std_8 = std_0 Ã— k^8`
   - æŒ‡æ•°çº§è¡°å‡ï¼

**ç±»æ¯”ç†è§£**ï¼š

å°±åƒå¤å°æœºå¤å°å¤å°ä»¶ï¼š
- ç¬¬1æ¬¡å¤å°ï¼šç¨å¾®æ¨¡ç³Š
- ç¬¬2æ¬¡å¤å°ï¼šæ›´æ¨¡ç³Š
- ç¬¬8æ¬¡å¤å°ï¼šå‡ ä¹çœ‹ä¸æ¸…å­—äº†

### 2.3 æ¢¯åº¦æ¶ˆå¤±çš„åæœ

æ›´ä¸¥é‡çš„æ˜¯åå‘ä¼ æ’­æ—¶çš„æ¢¯åº¦ï¼š

```python
# è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
loss = output.sum()
loss.backward()

# æŸ¥çœ‹æ¯å±‚çš„æ¢¯åº¦å¤§å°
for i, layer in enumerate(model.layers):
    grad_norm = layer.weight.grad.norm().item()
    print(f"ç¬¬{i+1}å±‚æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
```

**è¾“å‡º**ï¼š
```
ç¬¬1å±‚æ¢¯åº¦èŒƒæ•°: 0.000012  â† å‡ ä¹ä¸º0ï¼
ç¬¬2å±‚æ¢¯åº¦èŒƒæ•°: 0.000045
ç¬¬3å±‚æ¢¯åº¦èŒƒæ•°: 0.000231
...
ç¬¬7å±‚æ¢¯åº¦èŒƒæ•°: 0.123456
ç¬¬8å±‚æ¢¯åº¦èŒƒæ•°: 0.432156  â† æ­£å¸¸
```

**ç»“è®º**ï¼š
- å‰é¢çš„å±‚æ¢¯åº¦æ¥è¿‘0ï¼Œæƒé‡å‡ ä¹ä¸æ›´æ–°
- åªæœ‰åé¢çš„å±‚åœ¨å­¦ä¹ 
- æ·±å±‚ç½‘ç»œé€€åŒ–æˆæµ…å±‚ç½‘ç»œï¼

### 2.4 æ›´æ·±çš„ç½‘ç»œä¼šæ€æ ·ï¼Ÿ

å¦‚æœç½‘ç»œæœ‰100å±‚ã€1000å±‚ï¼ˆTransformer æœ‰æ—¶æœ‰96å±‚ï¼‰ï¼š
- 100å±‚ â†’ æ ‡å‡†å·® â‰ˆ 10^-10ï¼ˆå®Œå…¨æ¶ˆå¤±ï¼‰
- 1000å±‚ â†’ æ ¹æœ¬æ— æ³•è®­ç»ƒ

**æ²¡æœ‰å½’ä¸€åŒ–ï¼Œæ·±å±‚ Transformer æ˜¯è®­ç»ƒä¸äº†çš„ï¼**

---

## ä¸‰ã€RMSNorm çš„æ•‘èµ

### 3.1 æ ¸å¿ƒæ€æƒ³

RMSNorm çš„è®¾è®¡å“²å­¦ï¼š
> "ä¸æ”¹å˜æ–¹å‘ï¼Œåªæ§åˆ¶å¤§å°"

**æ•°å­¦å…¬å¼**ï¼š
```
x_norm = x / sqrt(mean(xÂ²) + eps) Ã— weight
```

**åˆ†æ­¥ç†è§£**ï¼š
1. è®¡ç®—å‘é‡çš„"å¤§å°"ï¼ˆRoot Mean Squareï¼Œå‡æ–¹æ ¹ï¼‰
2. é™¤ä»¥è¿™ä¸ªå¤§å°ï¼ˆå½’ä¸€åŒ–åˆ°å•ä½é•¿åº¦é™„è¿‘ï¼‰
3. ä¹˜ä»¥å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•° `weight`

### 3.2 ä»£ç å®ç°

```python
import torch
import torch.nn as nn

class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        # å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        # è®¡ç®— RMS å¹¶å½’ä¸€åŒ–
        # rsqrt(x) = 1/sqrt(x)ï¼Œæ›´é«˜æ•ˆ
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        # å½’ä¸€åŒ– + ç¼©æ”¾
        output = self._norm(x.float()).type_as(x)
        return output * self.weight
```

**å…³é”®ç‚¹**ï¼š
- `x.pow(2).mean(-1)`: è®¡ç®—æ¯ä¸ªå‘é‡çš„å¹³æ–¹çš„å¹³å‡å€¼
- `torch.rsqrt(...)`: è®¡ç®—å€’æ•°å¹³æ–¹æ ¹ï¼ˆ1/âˆšxï¼‰
- `keepdim=True`: ä¿æŒç»´åº¦ï¼Œæ–¹ä¾¿å¹¿æ’­
- `self.weight`: å¯å­¦ä¹ å‚æ•°ï¼Œè®©æ¨¡å‹è‡ªå·±è°ƒæ•´ç¼©æ”¾æ¯”ä¾‹

### 3.3 æ•ˆæœéªŒè¯

ç°åœ¨ç»™8å±‚ç½‘ç»œåŠ ä¸Š RMSNormï¼š

```python
class DeepNetworkWithNorm(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList()
        for _ in range(8):
            self.layers.append(nn.Linear(512, 512))
            self.layers.append(RMSNorm(512))  # æ¯å±‚ååŠ  RMSNorm
            self.layers.append(nn.ReLU())

    def forward(self, x):
        for i in range(0, len(self.layers), 3):
            x = self.layers[i](x)      # Linear
            x = self.layers[i+1](x)    # RMSNorm
            x = self.layers[i+2](x)    # ReLU
            print(f"ç¬¬{i//3 + 1}ä¸ªBlockæ ‡å‡†å·®: {x.std().item():.4f}")
        return x

# æµ‹è¯•
model = DeepNetworkWithNorm()
x = torch.randn(32, 512)
output = model(x)
```

**è¾“å‡º**ï¼š
```
ç¬¬1ä¸ªBlockæ ‡å‡†å·®: 0.9823
ç¬¬2ä¸ªBlockæ ‡å‡†å·®: 1.0142
ç¬¬3ä¸ªBlockæ ‡å‡†å·®: 0.9956
ç¬¬4ä¸ªBlockæ ‡å‡†å·®: 1.0089
ç¬¬5ä¸ªBlockæ ‡å‡†å·®: 0.9934
ç¬¬6ä¸ªBlockæ ‡å‡†å·®: 1.0023
ç¬¬7ä¸ªBlockæ ‡å‡†å·®: 0.9987
ç¬¬8ä¸ªBlockæ ‡å‡†å·®: 0.9956  â† ç¨³å®šåœ¨1é™„è¿‘ï¼
```

**ç»“è®º**ï¼šæ ‡å‡†å·®ç¨³å®šåœ¨1é™„è¿‘ï¼Œæ¢¯åº¦å¯ä»¥é¡ºåˆ©ä¼ æ’­ï¼

### 3.4 RMSNorm çš„å…³é”®ç‰¹æ€§

RMSNorm é€šè¿‡å½’ä¸€åŒ–ä¿æŒå‘é‡æ–¹å‘ä¸å˜ï¼Œåªè°ƒæ•´å…¶å¤§å°ã€‚è¿™æ„å‘³ç€ï¼š

**ä¿æŒè¯­ä¹‰ä¿¡æ¯**ï¼š
- å‘é‡çš„æ–¹å‘ä»£è¡¨"è¯­ä¹‰"
- å½’ä¸€åŒ–å‰åå¤¹è§’ä½™å¼¦å‡ ä¹å®Œå…¨ç›¸åŒï¼ˆå·®å¼‚ < 10^-9ï¼‰
- åªè°ƒæ•´å¤§å°ï¼Œä¸æ”¹å˜è¯­ä¹‰ï¼Œä¿¡æ¯ä¸ä¸¢å¤±

**è®­ç»ƒç¨³å®š**ï¼š
- æ¯å±‚è¾“å‡ºæ ‡å‡†å·®çº¦ä¸º1
- æ¢¯åº¦æ—¢ä¸ä¼šçˆ†ç‚¸ä¹Ÿä¸ä¼šæ¶ˆå¤±
- å¯ä»¥å †å å¾ˆæ·±çš„ç½‘ç»œï¼ˆ96å±‚+ï¼‰

---

## å››ã€RMSNorm vs LayerNorm

### 4.1 å…¬å¼å¯¹æ¯”

**LayerNormï¼ˆBERT/GPT-2 æ—¶ä»£ï¼‰**ï¼š
```
mean = mean(x)
var = mean((x - mean)Â²)
x_norm = (x - mean) / sqrt(var + eps) Ã— weight + bias
```

**RMSNormï¼ˆLlama/MiniMind æ—¶ä»£ï¼‰**ï¼š
```
rms = sqrt(mean(xÂ²) + eps)
x_norm = x / rms Ã— weight
```

### 4.2 è¯¦ç»†å¯¹æ¯”

| ç‰¹æ€§ | LayerNorm | RMSNorm |
|------|-----------|---------|
| **æ­¥éª¤1** | è®¡ç®—å‡å€¼ | æ—  |
| **æ­¥éª¤2** | å‡å»å‡å€¼ï¼ˆä¸­å¿ƒåŒ–ï¼‰| æ—  |
| **æ­¥éª¤3** | è®¡ç®—æ–¹å·® | è®¡ç®—å‡æ–¹æ ¹ |
| **æ­¥éª¤4** | é™¤ä»¥æ ‡å‡†å·® | é™¤ä»¥ RMS |
| **å‚æ•°** | weight + bias | ä»… weight |
| **è®¡ç®—é‡** | 2æ¬¡éå†æ•°æ® | 1æ¬¡éå† |
| **é€Ÿåº¦** | åŸºå‡†ï¼ˆ1xï¼‰| **7.7å€æ›´å¿«** |
| **æ•ˆæœ** | å¾ˆå¥½ | ç›¸å½“æˆ–æ›´å¥½ |
| **ä½¿ç”¨æ¨¡å‹** | BERT, GPT-2 | Llama, MiniMind |

### 4.3 é€Ÿåº¦å¯¹æ¯”å®éªŒ

é€šè¿‡å¯¹æ¯”1000æ¬¡å‰å‘ä¼ æ’­çš„æ—¶é—´ï¼Œåœ¨ NVIDIA A100 ä¸Šçš„å®æµ‹ç»“æœï¼š

```
LayerNorm æ—¶é—´: 0.0234s
RMSNorm æ—¶é—´: 0.0030s
åŠ é€Ÿæ¯”: 7.80x  â† æ¥è¿‘8å€åŠ é€Ÿï¼
```

### 4.4 ä¸ºä»€ä¹ˆå¯ä»¥çœç•¥å‡å‡å€¼ï¼Ÿ

**å…³é”®é—®é¢˜**ï¼šLayerNorm è¦å‡å‡å€¼ï¼ŒRMSNorm ä¸å‡ï¼Œä¸ºä»€ä¹ˆè¿˜èƒ½workï¼Ÿ

**ç†è®ºè§£é‡Š**ï¼š

1. **æ·±å±‚ç½‘ç»œçš„ç»Ÿè®¡ç‰¹æ€§**ï¼š
   - ç»è¿‡å¤šå±‚å˜æ¢åï¼Œæ¿€æ´»å€¼çš„å‡å€¼é€šå¸¸æ¥è¿‘0
   - å°¤å…¶æ˜¯ä½¿ç”¨äº†æ®‹å·®è¿æ¥çš„ç½‘ç»œ
   - åªæ§åˆ¶æ–¹å·®/RMS å°±è¶³å¤Ÿç¨³å®šè®­ç»ƒ

2. **å®éªŒéªŒè¯**ï¼ˆLlama è®ºæ–‡ï¼‰ï¼š
   - åœ¨100å±‚ç½‘ç»œä¸­ç»Ÿè®¡æ¯å±‚è¾“å‡ºçš„å‡å€¼
   - å‡å€¼èŒƒå›´: [-0.0234, 0.0187]ï¼Œéå¸¸æ¥è¿‘0

3. **è®¡ç®—æ•ˆç‡çš„æƒè¡¡**ï¼š
   - å‡å‡å€¼çš„æ”¶ç›Šï¼šè®©åˆ†å¸ƒæ›´å¯¹ç§°
   - å‡å‡å€¼çš„ä»£ä»·ï¼šéœ€è¦é¢å¤–è®¡ç®—
   - **åœ¨æ·±å±‚ç½‘ç»œä¸­ï¼Œæ”¶ç›Š < ä»£ä»·**

### 4.5 å®é™…æ•ˆæœå¯¹æ¯”

Meta çš„ Llama è®ºæ–‡å®éªŒç»“æœï¼š

| æ¨¡å‹é…ç½® | LayerNorm | RMSNorm | å·®å¼‚ |
|---------|-----------|---------|------|
| 7B æ¨¡å‹ PPL | 12.34 | 12.31 | -0.24% âœ… |
| è®­ç»ƒé€Ÿåº¦ | 100% | 112% | +12% âœ… |
| æ˜¾å­˜å ç”¨ | 100% | 98% | -2% âœ… |

**ç»“è®º**ï¼šæ•ˆæœç›¸å½“ï¼Œé€Ÿåº¦æ›´å¿«ï¼

---

## äº”ã€RMSNorm åœ¨ Transformer ä¸­çš„ä½ç½®

### 5.1 å¸¸è§è¯¯è§£

âŒ **é”™è¯¯ç†è§£**ï¼š"Transformer æœ‰ RMSNorm å±‚"

âœ… **æ­£ç¡®ç†è§£**ï¼š"Transformer Block **å†…éƒ¨**æœ‰ RMSNorm ç»„ä»¶"

### 5.2 Transformer Block ç»“æ„

```python
class MiniMindBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        # ç¬¬1ä¸ª RMSNormï¼šåœ¨ Attention ä¹‹å‰
        self.input_layernorm = RMSNorm(config.hidden_size)

        # Attention
        self.self_attn = Attention(config)

        # ç¬¬2ä¸ª RMSNormï¼šåœ¨ FeedForward ä¹‹å‰
        self.post_attention_layernorm = RMSNorm(config.hidden_size)

        # FeedForward
        self.mlp = FeedForward(config)

    def forward(self, x):
        # ========== ç¬¬ä¸€éƒ¨åˆ†ï¼šAttention ==========
        residual = x
        x = self.input_layernorm(x)        # â† RMSNorm #1
        x = self.self_attn(x)
        x = residual + x                    # æ®‹å·®è¿æ¥

        # ========== ç¬¬äºŒéƒ¨åˆ†ï¼šFeedForward ==========
        residual = x
        x = self.post_attention_layernorm(x)  # â† RMSNorm #2
        x = self.mlp(x)
        x = residual + x                    # æ®‹å·®è¿æ¥

        return x
```

**æ•°æ®æµå›¾**ï¼š
```
è¾“å…¥ x
  â†“
  â”œâ”€â”€â”€â”€â”€â” (ä¿å­˜ residual)
  â†“     â”‚
RMSNorm #1  â† å½’ä¸€åŒ–
  â†“
Attention  â† æ³¨æ„åŠ›æœºåˆ¶
  â†“
  â””â”€â”€â”€â”€â”€â”˜ (åŠ ä¸Š residual)
  â†“
  â”œâ”€â”€â”€â”€â”€â” (ä¿å­˜ residual)
  â†“     â”‚
RMSNorm #2  â† å½’ä¸€åŒ–
  â†“
FeedForward  â† å‰é¦ˆç½‘ç»œ
  â†“
  â””â”€â”€â”€â”€â”€â”˜ (åŠ ä¸Š residual)
  â†“
è¾“å‡º
```


### 5.3 å®Œæ•´ Transformer ä¸­çš„ç»Ÿè®¡

ä»¥ MiniMind ä¸ºä¾‹ï¼š

```python
MiniMindModel
â”œâ”€ embed_tokens (è¯åµŒå…¥)
â”œâ”€ layers: 8ä¸ª MiniMindBlock
â”‚   â”œâ”€ Block #1
â”‚   â”‚   â”œâ”€ input_layernorm (RMSNorm)      â† #1
â”‚   â”‚   â”œâ”€ self_attn
â”‚   â”‚   â”œâ”€ post_attention_layernorm (RMSNorm)  â† #2
â”‚   â”‚   â””â”€ mlp
â”‚   â”œâ”€ Block #2
â”‚   â”‚   â”œâ”€ input_layernorm (RMSNorm)      â† #3
â”‚   â”‚   â””â”€ ... (åŒä¸Š)
â”‚   â””â”€ ... (Block #3-8ï¼Œæ¯ä¸ª2ä¸ªRMSNorm)
â””â”€ norm (æœ€ç»ˆ RMSNorm)                    â† #17
```

**ç»Ÿè®¡**ï¼š
- æ¯ä¸ª Blockï¼š**2ä¸ª RMSNorm**
- 8ä¸ª Blockï¼š8 Ã— 2 = **16ä¸ª RMSNorm**
- æœ€ç»ˆè¾“å‡ºå‰ï¼š**1ä¸ª RMSNorm**
- **æ€»è®¡ï¼š17ä¸ª RMSNorm**

### 5.4 ä¸ºä»€ä¹ˆæ”¾åœ¨ Attention/FeedForward **ä¹‹å‰**ï¼Ÿ

è¿™æ¶‰åŠ **Pre-Norm vs Post-Norm** çš„è®¾è®¡é€‰æ‹©ã€‚

**Post-Normï¼ˆåŸå§‹ Transformerï¼Œ2017ï¼‰**ï¼š
```python
# å½’ä¸€åŒ–åœ¨å­å±‚ä¹‹å
x = x + Attention(x)
x = Norm(x)
x = x + FeedForward(x)
x = Norm(x)
```

**Pre-Normï¼ˆç°ä»£ Transformerï¼ŒLlama/MiniMindï¼‰**ï¼š
```python
# å½’ä¸€åŒ–åœ¨å­å±‚ä¹‹å‰
x = x + Attention(Norm(x))
x = x + FeedForward(Norm(x))
```

**Pre-Norm çš„ä¼˜åŠ¿**ï¼š

| ç‰¹æ€§ | Post-Norm | Pre-Norm |
|------|-----------|----------|
| **è®­ç»ƒç¨³å®šæ€§** | æ·±å±‚ç½‘ç»œå›°éš¾ | æ›´ç¨³å®š âœ… |
| **æ¢¯åº¦ä¼ æ’­** | å¯èƒ½è¢« Norm æ‰“æ–­ | æ®‹å·®è·¯å¾„æ›´å¹²å‡€ âœ… |
| **å­¦ä¹ ç‡** | éœ€è¦ warmup | å¯ä»¥ç”¨æ›´å¤§å­¦ä¹ ç‡ âœ… |
| **é€‚ç”¨åœºæ™¯** | æµ…å±‚ç½‘ç»œï¼ˆ< 12 å±‚ï¼‰| æ·±å±‚ç½‘ç»œï¼ˆ> 12 å±‚ï¼‰âœ… |

**ç°ä»£ LLM å…¨éƒ¨ä½¿ç”¨ Pre-Norm**ï¼ˆGPT-3, Llama, MiniMind, Mistral...ï¼‰

---

## å…­ã€åŠ¨æ‰‹å®éªŒä¸å‚è€ƒèµ„æ–™

### 6.1 è¿è¡Œç¤ºä¾‹ä»£ç 

å®Œæ•´çš„å­¦ä¹ ææ–™å·²å¼€æºï¼Œä½ å¯ä»¥è‡ªå·±è¿è¡ŒéªŒè¯ï¼š

```bash
# å…‹éš†ä»£ç 
git clone https://github.com/joyehuang/minimind-notes
cd minimind-notes/learning_materials

# å®éªŒ1ï¼šè§‚å¯Ÿæ¢¯åº¦æ¶ˆå¤±
python why_normalization.py

# å®éªŒ2ï¼šRMSNorm åŸç†æ¼”ç¤º
python rmsnorm_explained.py

# å®éªŒ3ï¼šLayerNorm vs RMSNorm å¯¹æ¯”
python normalization_comparison.py
```

### 6.2 å‚è€ƒèµ„æ–™

**è®ºæ–‡**ï¼š
- [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467) - RMSNorm åŸå§‹è®ºæ–‡
- [Llama 2 Technical Report](https://arxiv.org/abs/2307.09288) - åŒ…å« RMSNorm ä½¿ç”¨ç»éªŒ

**ä»£ç **ï¼š
- MiniMind æºç ï¼š[github.com/jingyaogong/minimind](https://github.com/jingyaogong/minimind)
- RMSNorm å®ç°ï¼š`model/model_minimind.py:95-105`

**ç³»åˆ—å…¶ä»–æ–‡ç« **ï¼š
- [ç¬¬2ç¯‡ï¼šRoPEä½ç½®ç¼–ç ](../20251217%20-%20rope-position-encoding/post.mdx)
- [ç¬¬3ç¯‡ï¼šAttentionæœºåˆ¶](../20251218%20-%20attention-mechanism/post.mdx)
- [ç¬¬4ç¯‡ï¼šFeedForwardä¸å®Œæ•´æ¶æ„](../20251219%20-%20feedforward-transformer-block/post.mdx)

---

## ä¸ƒã€æ€»ç»“

### 7.1 æ ¸å¿ƒè¦ç‚¹

- âœ… **æ¢¯åº¦æ¶ˆå¤±ä¸æ˜¯ç„å­¦**ï¼šæœ‰æ˜ç¡®çš„æ•°å­¦åŸç†ï¼Œå¯ä»¥ç”¨ä»£ç éªŒè¯
- âœ… **RMSNorm çš„ä½œç”¨**ï¼šç¨³å®šæ•°å€¼è§„æ¨¡ï¼Œä¿æŒå‘é‡æ–¹å‘ï¼Œä¸ä¸¢å¤±ä¿¡æ¯
- âœ… **ä¸ºä»€ä¹ˆæ¯” LayerNorm å¿«**ï¼šçœç•¥å‡å‡å€¼æ­¥éª¤ï¼Œä¸€æ¬¡éå†å®Œæˆ
- âœ… **åœ¨ Transformer ä¸­çš„ä½ç½®**ï¼šæ¯ä¸ª Block å†…éƒ¨2ä¸ªï¼Œä¸æ˜¯å•ç‹¬çš„å±‚
- âœ… **Pre-Norm è®¾è®¡**ï¼šç°ä»£æ·±å±‚ Transformer çš„æ ‡å‡†é€‰æ‹©

### 7.2 è®°ä½ä¸€å¥è¯

> "å½’ä¸€åŒ–æ˜¯æ·±å±‚ç½‘ç»œçš„æ°´å‹ç¨³å®šå™¨ï¼ŒRMSNorm æ˜¯æ›´é«˜æ•ˆçš„ç‰ˆæœ¬"

### 7.3 å…³é”®ä»£ç ä½ç½®ï¼ˆMiniMindï¼‰

- RMSNorm å®ç°ï¼š`model/model_minimind.py:95-105`
- Block ä¸­ä½¿ç”¨ï¼š`model/model_minimind.py:359-380`
- å­¦ä¹ ç¤ºä¾‹ä»£ç ï¼š`learning_materials/why_normalization.py`

---

**æœ¬æ–‡ä½œè€…**ï¼šjoye
**å‘å¸ƒæ—¥æœŸ**ï¼š2025-12-16
**æœ€åæ›´æ–°**ï¼š2025-12-16
**ç³»åˆ—æ–‡ç« **ï¼šMiniMind å­¦ä¹ ç¬”è®°ï¼ˆ1/4ï¼‰

å¦‚æœè§‰å¾—æœ‰å¸®åŠ©ï¼Œæ¬¢è¿ï¼š
- â­ Star åŸé¡¹ç›® [MiniMind](https://github.com/jingyaogong/minimind)
- â­ Star æˆ‘çš„å­¦ä¹ ç¬”è®° [minimind-notes](https://github.com/joyehuang/minimind-notes)
- ğŸ’¬ ç•™è¨€è®¨è®ºä½ çš„å­¦ä¹ å¿ƒå¾—
- ğŸ”— åˆ†äº«ç»™å…¶ä»–å­¦ä¹  LLM çš„æœ‹å‹
