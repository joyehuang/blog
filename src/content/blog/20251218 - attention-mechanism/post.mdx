---
title: æ·±å…¥ç†è§£Attentionæœºåˆ¶ï¼šä»Qã€Kã€Våˆ°Multi-Head
description: 'æ·±å…¥è§£æ Attentionï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰â€”â€” Transformer çš„æ ¸å¿ƒå¼•æ“ã€‚ç”¨æ•°æ®åº“æŸ¥è¯¢çš„ç±»æ¯”ï¼Œå½»åº•ç†è§£ Qã€Kã€V çš„å«ä¹‰ï¼ŒæŒæ¡ Multi-Head Attention çš„å®ç°ï¼Œå¹¶æ¾„æ¸… Softmax ä¸ RMSNorm çš„å¸¸è§æ··æ·†'
publishDate: 2025-12-18 00:00:00
tags: ['LLM', 'Transformer', 'MiniMind', 'Attention', 'Multi-Head']
---

> æœ¬æ–‡æ˜¯ MiniMind å­¦ä¹ ç³»åˆ—çš„ç¬¬3ç¯‡ï¼Œæ·±å…¥è§£æ Attentionï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰â€”â€” Transformer çš„æ ¸å¿ƒå¼•æ“ã€‚æˆ‘ä»¬å°†ç”¨æ•°æ®åº“æŸ¥è¯¢çš„ç±»æ¯”ï¼Œè®©ä½ å½»åº•ç†è§£ Qã€Kã€V çš„å«ä¹‰ï¼ŒæŒæ¡ Multi-Head Attention çš„å®ç°ï¼Œå¹¶æ¾„æ¸… Softmax ä¸ RMSNorm çš„å¸¸è§æ··æ·†ã€‚

## å…³äºæœ¬ç³»åˆ—

[MiniMind](https://github.com/jingyaogong/minimind) æ˜¯ä¸€ä¸ªç®€æ´ä½†å®Œæ•´çš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒé¡¹ç›®ï¼ŒåŒ…å«ä»æ•°æ®å¤„ç†ã€æ¨¡å‹è®­ç»ƒåˆ°æ¨ç†éƒ¨ç½²çš„å®Œæ•´æµç¨‹ã€‚æˆ‘åœ¨å­¦ä¹ è¿™ä¸ªé¡¹ç›®çš„è¿‡ç¨‹ä¸­ï¼Œå°†æ ¸å¿ƒæŠ€æœ¯ç‚¹æ•´ç†æˆäº† [minimind-notes](https://github.com/joyehuang/minimind-notes) ä»“åº“ï¼Œå¹¶äº§å‡ºäº†è¿™ä¸ª4ç¯‡ç³»åˆ—åšå®¢ï¼Œç³»ç»Ÿæ€§åœ°è®²è§£ Transformer çš„æ ¸å¿ƒç»„ä»¶ã€‚

æœ¬ç³»åˆ—åŒ…æ‹¬ï¼š
1. **å½’ä¸€åŒ–æœºåˆ¶** - ä¸ºä»€ä¹ˆéœ€è¦RMSNorm
2. **RoPEä½ç½®ç¼–ç ** - å¦‚ä½•è®©æ¨¡å‹ç†è§£è¯åº
3. **Attentionæœºåˆ¶**ï¼ˆæœ¬ç¯‡ï¼‰- Transformerçš„æ ¸å¿ƒå¼•æ“
4. **FeedForwardä¸å®Œæ•´æ¶æ„** - ç»„ä»¶å¦‚ä½•ååŒå·¥ä½œ

---

## ä¸€ã€å¼•è¨€

### 1.1 Transformer çš„çµé­‚

å¦‚æœè¯´ Transformer æ˜¯ä¸€åº§å¤§å¦ï¼š
- **å½’ä¸€åŒ–ï¼ˆRMSNormï¼‰**æ˜¯åœ°åŸº â€” ç¨³å®šè®­ç»ƒ
- **ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰**æ˜¯åæ ‡ç³» â€” åŒºåˆ†ä½ç½®
- **Attention** æ˜¯æ ¸å¿ƒå¼•æ“ â€” ç†è§£è¯­ä¹‰ â­

æ²¡æœ‰ Attentionï¼ŒTransformer å°±ä¸å­˜åœ¨ã€‚

### 1.2 æœ¬æ–‡è¦è§£ç­”çš„é—®é¢˜

- Qã€Kã€V åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆä¸æ˜¯ç„å­¦ï¼ï¼‰
- ä¸ºä»€ä¹ˆè¦åˆ†æˆ 8 ä¸ª Headï¼Ÿ
- Softmax å’Œ RMSNorm æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿï¼ˆå¸¸è§æ··æ·†ï¼‰
- Attention å¦‚ä½•ä¸ RoPE é…åˆå·¥ä½œï¼Ÿ
- Multi-Head çš„ç»´åº¦å˜åŒ–æ˜¯æ€æ ·çš„ï¼Ÿ

### 1.3 é€‚åˆè¯»è€…

- å¬è¯´è¿‡ Attention ä½†ä¸ç†è§£è®¡ç®—ç»†èŠ‚
- æƒ³ä»ä»£ç å±‚é¢æŒæ¡ Multi-Head æœºåˆ¶
- å‡†å¤‡å®ç°è‡ªå·±çš„ Transformer
- å¯¹æ•°å­¦æ¨å¯¼ä¸ææƒ§ï¼ˆæœ¬æ–‡ä¼šè¯¦ç»†è§£é‡Šï¼‰

---

## äºŒã€Attention çš„æœ¬è´¨ï¼šè¯ä¸è¯çš„ç›¸å…³æ€§

### 2.1 æ ¸å¿ƒé—®é¢˜

> "åœ¨ç†è§£ä¸€ä¸ªè¯æ—¶ï¼Œåº”è¯¥å…³æ³¨å¥å­ä¸­çš„å“ªäº›å…¶ä»–è¯ï¼Ÿ"

**ä¾‹å­**ï¼š
```
å¥å­: "å°æ˜å¾ˆå–œæ¬¢ä»–çš„çŒ«ï¼Œå®ƒæ€»æ˜¯åœ¨çª—è¾¹ç¡è§‰"

å½“æ¨¡å‹ç†è§£"å®ƒ"è¿™ä¸ªè¯æ—¶ï¼š
  "å®ƒ" â† "å°æ˜"  ç›¸å…³æ€§: 0.1  ï¼ˆå¯èƒ½æ€§ä½ï¼Œä»£è¯é€šå¸¸ä¸æŒ‡äººåï¼‰
  "å®ƒ" â† "å–œæ¬¢"  ç›¸å…³æ€§: 0.05 ï¼ˆå‡ ä¹æ— å…³ï¼‰
  "å®ƒ" â† "çŒ«"    ç›¸å…³æ€§: 0.8  ï¼ˆé«˜åº¦ç›¸å…³ï¼ï¼‰âœ…
  "å®ƒ" â† "çª—è¾¹"  ç›¸å…³æ€§: 0.05 ï¼ˆå‡ ä¹æ— å…³ï¼‰

æœ€ç»ˆç†è§£"å®ƒ"çš„è¡¨ç¤º = 0.1Ã—[å°æ˜] + 0.05Ã—[å–œæ¬¢] + 0.8Ã—[çŒ«] + 0.05Ã—[çª—è¾¹]
                      â‰ˆ ä¸»è¦æ¥è‡ª"çŒ«"çš„ä¿¡æ¯
```

**Attention åšçš„äº‹æƒ…**ï¼š
1. **è®¡ç®—ç›¸å…³æ€§åˆ†æ•°**ï¼ˆæ¯ä¸¤ä¸ªè¯ä¹‹é—´ï¼‰
2. **å½’ä¸€åŒ–æˆæ¦‚ç‡åˆ†å¸ƒ**ï¼ˆSoftmaxï¼ŒåŠ èµ·æ¥ = 1ï¼‰
3. **åŠ æƒæ±‚å’Œ**ï¼ˆèåˆä¸Šä¸‹æ–‡ï¼‰


### 2.2 è¾“å…¥è¾“å‡ºå¯¹æ¯”

```python
# è¾“å…¥ï¼šå­¤ç«‹çš„è¯å‘é‡ï¼ˆæ¯ä¸ªè¯ä¸çŸ¥é“ä¸Šä¸‹æ–‡ï¼‰
input = [
    [æˆ‘çš„768ç»´å‘é‡],     # ä¸çŸ¥é“åé¢æ˜¯"çˆ±"è¿˜æ˜¯"æ¨"
    [çˆ±çš„768ç»´å‘é‡],     # ä¸çŸ¥é“ä¸»è¯­æ˜¯è°ã€å®¾è¯­æ˜¯è°
    [ç¼–ç¨‹çš„768ç»´å‘é‡]    # ä¸çŸ¥é“æ˜¯è¢«çˆ±è¿˜æ˜¯è¢«æ¨
]

# Attention å¤„ç†

# è¾“å‡ºï¼šèåˆäº†ä¸Šä¸‹æ–‡çš„è¯å‘é‡
output = [
    [æˆ‘çš„æ–°å‘é‡],  # ç°åœ¨çŸ¥é“ï¼šæˆ‘åœ¨"çˆ±"è¿™ä¸ªåŠ¨ä½œä¸­æ˜¯ä¸»è¯­
    [çˆ±çš„æ–°å‘é‡],  # ç°åœ¨çŸ¥é“ï¼šè¿æ¥"æˆ‘"å’Œ"ç¼–ç¨‹"
    [ç¼–ç¨‹çš„æ–°å‘é‡] # ç°åœ¨çŸ¥é“ï¼šåœ¨"çˆ±"è¿™ä¸ªåŠ¨ä½œä¸­æ˜¯å®¾è¯­
]
```

### 2.3 Self-Attention vs Cross-Attention

**Self-Attentionï¼ˆMiniMind ä½¿ç”¨ï¼‰**ï¼š
```python
# å¥å­å…³æ³¨"è‡ªå·±å†…éƒ¨"çš„è¯
sentence = "æˆ‘çˆ±ç¼–ç¨‹"
# è®¡ç®—ï¼šæˆ‘ â† â†’ çˆ± â† â†’ ç¼–ç¨‹ çš„ç›¸å…³æ€§
```

**Cross-Attentionï¼ˆç¿»è¯‘æ¨¡å‹ä½¿ç”¨ï¼‰**ï¼š
```python
# å¥å­ A å…³æ³¨å¥å­ B
chinese = "æˆ‘çˆ±ç¼–ç¨‹"
english = "I love programming"
# è®¡ç®—ï¼š"æˆ‘" â† "I"ï¼Œ"çˆ±" â† "love"ï¼Œ"ç¼–ç¨‹" â† "programming"
```

**ä¸ºä»€ä¹ˆå«"Self"**ï¼Ÿ
- å› ä¸ºè®¡ç®—çš„æ˜¯**åŒä¸€ä¸ªå¥å­å†…éƒ¨**çš„å…³ç³»
- ä¸æ˜¯"token ä¸è‡ªå·±"ï¼ˆè™½ç„¶ä¹Ÿä¼šè®¡ç®— q_i Â· k_iï¼‰

---

## ä¸‰ã€Qã€Kã€V è¯¦è§£ï¼šæ•°æ®åº“æŸ¥è¯¢ç±»æ¯”

### 3.1 ç»å…¸ç±»æ¯”ï¼šæ•°æ®åº“æŸ¥è¯¢

ç†è§£ Qã€Kã€V æœ€å¥½çš„æ–¹å¼æ˜¯ç±»æ¯” SQL æŸ¥è¯¢ï¼š

```sql
SELECT value          â† è¿”å› Value
FROM memory_bank      â† è®°å¿†åº“ï¼ˆæ‰€æœ‰è¯ï¼‰
WHERE key MATCHES query  â† Key åŒ¹é… Query
```

**å¯¹åº”å…³ç³»**ï¼š

| SQL æ¦‚å¿µ | Attention æ¦‚å¿µ | ä½œç”¨ | ç±»æ¯” |
|---------|---------------|------|------|
| **Query** | Query (Q) | "æˆ‘æƒ³æŸ¥è¯¢ä»€ä¹ˆä¿¡æ¯ï¼Ÿ" | æœç´¢æ¡ä»¶ |
| **Key** | Key (K) | "æˆ‘è¿™é‡Œæœ‰ä»€ä¹ˆä¿¡æ¯ï¼Ÿ" | ç´¢å¼•æ ‡ç­¾ |
| **Value** | Value (V) | "æˆ‘çš„å®é™…å†…å®¹" | æ•°æ®å€¼ |

### 3.2 å…·ä½“ä¾‹å­ï¼šç†è§£"çˆ±"

å¥å­ï¼š"æˆ‘ çˆ± ç¼–ç¨‹"

**å½“ç†è§£"çˆ±"è¿™ä¸ªè¯æ—¶**ï¼š

1. **Queryï¼ˆ"çˆ±"æƒ³çŸ¥é“ä»€ä¹ˆï¼‰**ï¼šä¸»è¯­å’Œå®¾è¯­æ˜¯è°ï¼Ÿæˆ‘åœ¨è¡¨è¾¾ä»€ä¹ˆåŠ¨ä½œï¼Ÿ

2. **Keysï¼ˆå…¶ä»–è¯æä¾›ä»€ä¹ˆä¿¡æ¯ï¼‰**ï¼š
   - Key("æˆ‘") = "æˆ‘æ˜¯ä¸»è¯­ï¼Œç¬¬ä¸€äººç§°ä»£è¯"
   - Key("ç¼–ç¨‹") = "æˆ‘æ˜¯å®¾è¯­ï¼Œè¡¨ç¤ºæ´»åŠ¨"

3. **è®¡ç®—ç›¸ä¼¼åº¦**ï¼š
   - "çˆ±"çš„Q Â· "æˆ‘"çš„K = 0.6ï¼ˆä¸­ç­‰ç›¸å…³ï¼‰
   - "çˆ±"çš„Q Â· "ç¼–ç¨‹"çš„K = 0.8ï¼ˆé«˜åº¦ç›¸å…³ï¼ï¼‰

4. **Softmaxå½’ä¸€åŒ–**ï¼š`[0.25, 0.15, 0.60]`ï¼ˆå…³æ³¨"ç¼–ç¨‹"60%ï¼‰

5. **åŠ æƒæ±‚å’ŒValue**ï¼š
   - "çˆ±"çš„æ–°è¡¨ç¤º = 0.25Ã—Value("æˆ‘") + 0.15Ã—Value("çˆ±") + 0.60Ã—Value("ç¼–ç¨‹")
   - èåˆäº†ä¸Šä¸‹æ–‡ï¼ŒçŸ¥é“è‡ªå·±è¿æ¥"æˆ‘"å’Œ"ç¼–ç¨‹"


### 3.3 Qã€Kã€V æ€ä¹ˆå¾—åˆ°ï¼Ÿ

**å…³é”®å‘ç°**ï¼šQã€Kã€V éƒ½æ˜¯ä»**åŒä¸€ä¸ªè¾“å…¥ X** é€šè¿‡**ä¸åŒæƒé‡çŸ©é˜µ**å˜æ¢å¾—åˆ°ï¼

```python
# è¾“å…¥ X: [3, 768]ï¼ˆ3ä¸ªè¯ï¼Œæ¯ä¸ª768ç»´ï¼‰
# æƒé‡çŸ©é˜µ W_Q, W_K, W_V: [768, 768]

Q = X @ W_Q  # Query: "æˆ‘æƒ³çŸ¥é“ä»€ä¹ˆï¼Ÿ"
K = X @ W_K  # Key: "æˆ‘æœ‰ä»€ä¹ˆä¿¡æ¯ï¼Ÿ"
V = X @ W_V  # Value: "æˆ‘çš„å®é™…å†…å®¹"
```

**ç»´åº¦ç›¸åŒï¼Œå«ä¹‰ä¸åŒ**ã€‚ä¸‰ä¸ªçŸ©é˜µå°†è¾“å…¥å˜æ¢æˆä¸‰ä¸ªä¸åŒ"è§†è§’"ã€‚

### 3.4 æƒé‡çŸ©é˜µçš„æœ¬è´¨

**å¸¸è§ç–‘é—®**ï¼š"W_Qã€W_Kã€W_V ä»å“ªæ¥ï¼Ÿ"

**ç­”æ¡ˆ**ï¼š
- **æ˜¯ä»€ä¹ˆ**ï¼šç¥ç»ç½‘ç»œçš„å¯å­¦ä¹ å‚æ•°
- **æ€ä¹ˆæ¥**ï¼šé€šè¿‡è®­ç»ƒæ•°æ®åå‘ä¼ æ’­å­¦ä¹ 
- **å­˜åœ¨å“ªé‡Œ**ï¼šä¿å­˜åœ¨æ¨¡å‹æ–‡ä»¶é‡Œï¼ˆ.pth, .safetensorsï¼‰
- **ä½œç”¨**ï¼šæŠŠè¾“å…¥å˜æ¢æˆä¸‰ä¸ªä¸åŒ"è§†è§’"

åœ¨ MiniMind ä¸­ï¼Œå®ƒä»¬æ˜¯ä¸‰ä¸ª `nn.Linear` å±‚ï¼ˆ`q_proj`, `k_proj`, `v_proj`ï¼‰ã€‚è®­ç»ƒåï¼ŒW_Q å­¦ä¼šæå–"æŸ¥è¯¢ç‰¹å¾"ï¼ŒW_K å­¦ä¼šæå–"ç´¢å¼•ç‰¹å¾"ï¼ŒW_V å­¦ä¼šæå–"å†…å®¹ç‰¹å¾"ã€‚

---

## å››ã€Attention è®¡ç®—æµç¨‹

### 4.1 å®Œæ•´å…¬å¼

```
Attention(Q, K, V) = softmax(Q @ K^T / âˆšd_k) @ V
```

è¿™ä¸ªå…¬å¼æµ“ç¼©äº†æ•´ä¸ª Attention æœºåˆ¶ï¼

### 4.2 åˆ†æ­¥éª¤è¯¦è§£

**æ­¥éª¤ 1ï¼šè®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç‚¹ç§¯ï¼‰**
```python
scores = Q @ K.T  # [seq_len, seq_len]
# scores[i, j] = Q[i] Â· K[j]ï¼ˆä¸¤ä¸ªå‘é‡è¶Šç›¸ä¼¼ï¼Œç‚¹ç§¯è¶Šå¤§ï¼‰
```

**æ­¥éª¤ 2ï¼šç¼©æ”¾ï¼ˆé™¤ä»¥ âˆšd_kï¼‰**
```python
scaled_scores = scores / math.sqrt(head_dim)
```
**ä¸ºä»€ä¹ˆç¼©æ”¾**ï¼Ÿç»´åº¦è¶Šå¤§ï¼Œç‚¹ç§¯è¶Šå¤§ã€‚ä¸ç¼©æ”¾ä¼šå¯¼è‡´ Softmax å¤ª"å°–é”"ï¼Œæ¢¯åº¦æ¶ˆå¤±ã€‚ç¼©æ”¾ååˆ†å¸ƒæ›´å¹³æ»‘ï¼Œæ¢¯åº¦æ›´ç¨³å®šã€‚

**æ­¥éª¤ 3ï¼šSoftmax å½’ä¸€åŒ–**
```python
attn_weights = softmax(scaled_scores, dim=-1)
```
è½¬æ¢æˆæ¦‚ç‡åˆ†å¸ƒï¼šæ‰€æœ‰æƒé‡â‰¥0ï¼Œæ¯ä¸€è¡ŒåŠ èµ·æ¥=1ï¼Œå¯ä»¥è§£é‡Šä¸º"å…³æ³¨åº¦"ã€‚

**æ­¥éª¤ 4ï¼šåŠ æƒæ±‚å’Œ Value**
```python
output = attn_weights @ V
```
ä¾‹å¦‚ï¼š"çˆ±"çš„æ–°è¡¨ç¤º = 0.29Ã—Value("æˆ‘") + 0.36Ã—Value("çˆ±") + 0.25Ã—Value("ç¼–ç¨‹")


### 4.3 å®Œæ•´ä»£ç å®ç°

```python
def attention(Q, K, V, mask=None):
    head_dim = Q.shape[-1]

    # 1-2. è®¡ç®—ç›¸ä¼¼åº¦å¹¶ç¼©æ”¾
    scores = (Q @ K.transpose(-2, -1)) / math.sqrt(head_dim)

    # 3. åº”ç”¨æ©ç ï¼ˆå¯é€‰ï¼Œç”¨äºå› æœæ³¨æ„åŠ›ï¼‰
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    # 4-5. Softmax + åŠ æƒæ±‚å’Œ
    attn_weights = F.softmax(scores, dim=-1)
    output = attn_weights @ V

    return output, attn_weights
```

---

## äº”ã€Multi-Head Attention

### 5.1 ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´ï¼Ÿ

**å•å¤´çš„å±€é™**ï¼šåªèƒ½å…³æ³¨ä¸€ä¸ªæ–¹é¢ã€‚

å¥å­ï¼š"å°æ˜åœ¨åŒ—äº¬çš„æ¸…åå¤§å­¦å­¦ä¹ äººå·¥æ™ºèƒ½"

**å•å¤´ Attention å¯èƒ½åªå…³æ³¨**ï¼š
- ä¸»è°“å®¾å…³ç³»ï¼ˆè¯­æ³•ï¼‰

**ä½†æˆ‘ä»¬å¸Œæœ›åŒæ—¶å…³æ³¨**ï¼š
- è¯­æ³•ç»“æ„ï¼ˆä¸»è°“å®¾ï¼‰
- å®ä½“å…³ç³»ï¼ˆå°æ˜-æ¸…åï¼‰
- åœ°ç†ä½ç½®ï¼ˆæ¸…å-åŒ—äº¬ï¼‰
- ä¸»é¢˜é¢†åŸŸï¼ˆäººå·¥æ™ºèƒ½ï¼‰
- è¯­ä¹‰ç›¸å…³ï¼ˆå­¦ä¹ -äººå·¥æ™ºèƒ½ï¼‰
- ...

**è§£å†³æ–¹æ¡ˆ**ï¼šMulti-Head Attentionï¼

### 5.2 "å¤šå‰¯çœ¼é•œ"çš„ç±»æ¯”

```
Head 1: è¯­æ³•çœ¼é•œ ğŸ‘“
  â†’ å…³æ³¨ä¸»è°“å®¾å…³ç³»ã€å¥æ³•ç»“æ„

Head 2: å®ä½“çœ¼é•œ ğŸ•¶ï¸
  â†’ å…³æ³¨äººåã€åœ°åã€æœºæ„å

Head 3: è¯­ä¹‰çœ¼é•œ ğŸ‘“
  â†’ å…³æ³¨åŒä¹‰è¯ã€ç›¸å…³æ¦‚å¿µ

Head 4: é•¿è·ç¦»ä¾èµ–çœ¼é•œ ğŸ•¶ï¸
  â†’ å…³æ³¨è·ç¦»è¾ƒè¿œä½†ç›¸å…³çš„è¯

Head 5: æƒ…æ„Ÿçœ¼é•œ ğŸ‘“
  â†’ å…³æ³¨æƒ…æ„Ÿè¯ã€æ€åº¦è¯

...

Head 8: ä¸»é¢˜çœ¼é•œ ğŸ•¶ï¸
  â†’ å…³æ³¨ä¸»é¢˜å’Œé¢†åŸŸè¯æ±‡

æœ€åï¼šæ‘˜ä¸‹æ‰€æœ‰çœ¼é•œï¼Œèåˆ 8 ä¸ªè§†è§’ï¼
```


### 5.3 Multi-Head çš„å®ç°æµç¨‹

```python
# MiniMind é…ç½®
hidden_size = 768
num_heads = 8
head_dim = hidden_size // num_heads = 96

# å®Œæ•´æµç¨‹
è¾“å…¥ X: [batch, seq_len, 768]
  â†“
ç”Ÿæˆ Q, K, V: [batch, seq_len, 768]
  â†“
æ‹†åˆ†æˆ 8 ä¸ªå¤´: [batch, seq_len, 8, 96]
  â†“
è½¬ç½®: [batch, 8, seq_len, 96]  # æ–¹ä¾¿å¹¶è¡Œè®¡ç®—
  â†“
æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®— Attentionï¼ˆå¹¶è¡Œï¼‰
  â†“
è¾“å‡º: [batch, 8, seq_len, 96]
  â†“
è½¬å›: [batch, seq_len, 8, 96]
  â†“
åˆå¹¶ï¼ˆreshapeï¼‰: [batch, seq_len, 768]
  â†“
è¾“å‡ºæŠ•å½±: [batch, seq_len, 768]
```

### 5.4 ä»£ç å®ç°

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, hidden_size=768, num_heads=8):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads  # 96

        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.k_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.v_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.o_proj = nn.Linear(hidden_size, hidden_size, bias=False)

    def forward(self, x, mask=None):
        batch, seq_len, _ = x.shape

        # 1. ç”ŸæˆQã€Kã€Vå¹¶æ‹†åˆ†æˆå¤šå¤´
        Q = self.q_proj(x).view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.k_proj(x).view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.v_proj(x).view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        # [batch, num_heads, seq_len, head_dim]

        # 2. è®¡ç®—Attentionï¼ˆ8ä¸ªå¤´å¹¶è¡Œï¼‰
        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attn_weights = F.softmax(scores, dim=-1)
        output = attn_weights @ V

        # 3. åˆå¹¶å¤šå¤´å¹¶è¾“å‡ºæŠ•å½±
        output = output.transpose(1, 2).contiguous().view(batch, seq_len, -1)
        return self.o_proj(output)
```

### 5.5 ç»´åº¦è¿½è¸ª

```
è¾“å…¥: [batch, seq_len, 768]
  â†’ Qã€Kã€V: [batch, seq_len, 768]
  â†’ æ‹†åˆ†+è½¬ç½®: [batch, 8, seq_len, 96]
  â†’ Attention: [batch, 8, seq_len, 96]
  â†’ åˆå¹¶: [batch, seq_len, 768]
  â†’ è¾“å‡ºæŠ•å½±: [batch, seq_len, 768]

å…³é”®ä¸å˜é‡ï¼šnum_heads Ã— head_dim = 768
```

### 5.6 ä¸ºä»€ä¹ˆæ‹†åˆ†åè¦æ‹¼æ¥ï¼Ÿ

**æ‹†åˆ†**ï¼šè®©æ¯ä¸ªå¤´ä¸“æ³¨ä¸åŒæ–¹é¢
```python
# Head 1 å­¦ä¼šå…³æ³¨è¯­æ³•
# Head 2 å­¦ä¼šå…³æ³¨å®ä½“
# ...
```

**æ‹¼æ¥**ï¼šèåˆæ‰€æœ‰è§†è§’çš„ä¿¡æ¯
```python
# ç±»æ¯”ï¼š8 ä¸ªä¸“å®¶åˆ†åˆ«åˆ†æåŒä¸€ä¸ªæ¡ˆä¾‹
# æ¯ä¸ªä¸“å®¶å†™ä¸€ä»½ 96 å­—çš„æŠ¥å‘Š
# æœ€åæ‹¼æˆä¸€ä»½ 768 å­—çš„ç»¼åˆæŠ¥å‘Š
```

**ä¸ºä»€ä¹ˆä¸æ˜¯ç®€å•å¹³å‡ï¼Ÿ**
- æ‹¼æ¥ä¿ç•™äº†æ‰€æœ‰ä¿¡æ¯ï¼ˆ768 ç»´ï¼‰
- å¹³å‡ä¼šä¸¢å¤±ä¿¡æ¯ï¼ˆè¿˜æ˜¯ 96 ç»´ï¼‰
- åç»­çš„ FFN å¯ä»¥å­¦ä¹ å¦‚ä½•èåˆè¿™äº›ä¿¡æ¯

---

## å…­ã€å¸¸è§æ··æ·†ï¼šSoftmax vs RMSNorm

### 6.1 å¾ˆå¤šäººçš„ç–‘é—®

> "Attention é‡Œçš„ Softmax å’Œ Transformer Block é‡Œçš„ RMSNorm éƒ½æ˜¯å½’ä¸€åŒ–ï¼Œæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"

**è¿™æ˜¯ä¸ªå¸¸è§çš„æ··æ·†ï¼**

### 6.2 æ ¸å¿ƒåŒºåˆ«

| ç‰¹æ€§ | Softmaxï¼ˆAttention å†…éƒ¨ï¼‰| RMSNormï¼ˆBlock ä¹‹é—´ï¼‰|
|------|------------------------|---------------------|
| **ä½ç½®** | Attention è®¡ç®—**å†…éƒ¨** | Attention/FFN **ä¹‹å‰** |
| **å½’ä¸€åŒ–å¯¹è±¡** | ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆåˆ†æ•°çŸ©é˜µçš„æ¯ä¸€è¡Œï¼‰| è¯å‘é‡ï¼ˆæ¯ä¸ªå‘é‡çš„å¤§å°ï¼‰|
| **ç›®çš„** | å˜æˆæ¦‚ç‡åˆ†å¸ƒ | ç¨³å®šæ•°å€¼ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ |
| **è¾“å…¥** | ä»»æ„åˆ†æ•°ï¼ˆ-âˆ åˆ° +âˆï¼‰| 768 ç»´å‘é‡ |
| **è¾“å‡º** | 0-1 ä¹‹é—´ï¼Œå’Œä¸º 1 | å½’ä¸€åŒ–å‘é‡ï¼ˆæ–¹å‘ä¸å˜ï¼‰|
| **å…¬å¼** | `exp(x_i) / Î£exp(x_j)` | `x / sqrt(mean(xÂ²))` |
| **ä½œç”¨èŒƒå›´** | æ¯ä¸€è¡Œç‹¬ç«‹å½’ä¸€åŒ– | æ¯ä¸ªå‘é‡ç‹¬ç«‹å½’ä¸€åŒ– |

### 6.3 åœ¨ä»£ç ä¸­çš„ä½ç½®

```python
# Transformer Block
def forward(self, x):
    # ========== RMSNorm ==========
    residual = x
    x = self.input_norm(x)  # â† RMSNormï¼šå½’ä¸€åŒ–è¯å‘é‡

    # ========== Attention å†…éƒ¨ ==========
    Q, K, V = self.q_proj(x), self.k_proj(x), self.v_proj(x)

    # æ‹†åˆ†å¤šå¤´...

    scores = Q @ K.T
    weights = F.softmax(scores, dim=-1)  # â† Softmaxï¼šå½’ä¸€åŒ–åˆ†æ•°
    output = weights @ V

    # ==========  æ®‹å·®è¿æ¥ ==========
    x = residual + output

    return x
```

### 6.4 è¯¦ç»†å¯¹æ¯”ç¤ºä¾‹

**Softmax ç¤ºä¾‹**ï¼š
```python
# Attention åˆ†æ•°çŸ©é˜µçš„ä¸€è¡Œ
scores = torch.tensor([2.5, 1.3, 3.7, 0.8])

# Softmax å½’ä¸€åŒ–
weights = F.softmax(scores, dim=-1)
print(weights)
# è¾“å‡º: tensor([0.1722, 0.0518, 0.5678, 0.0082])
# ç‰¹ç‚¹ï¼š
# - æ‰€æœ‰å€¼åœ¨ [0, 1]
# - åŠ èµ·æ¥ = 1
# - å¤§çš„æ›´å¤§ï¼ˆ3.7 â†’ 0.5678ï¼Œå 56.78%ï¼‰
```

**RMSNorm ç¤ºä¾‹**ï¼š
```python
# ä¸€ä¸ªè¯å‘é‡
x = torch.tensor([2.5, 1.3, 3.7, 0.8])

# RMSNorm å½’ä¸€åŒ–
rms = torch.sqrt((x ** 2).mean())
x_norm = x / rms
print(x_norm)
# è¾“å‡º: tensor([1.0698, 0.5563, 1.5833, 0.3424])
# ç‰¹ç‚¹ï¼š
# - å€¼å¯ä»¥æ˜¯ä»»æ„æ­£è´Ÿæ•°
# - RMS â‰ˆ 1
# - æ–¹å‘ä¸å˜ï¼ˆåªç¼©æ”¾å¤§å°ï¼‰
```


### 6.5 è®°å¿†å£è¯€

```
Softmax: å½’ä¸€åŒ–"åˆ†æ•°åˆ†å¸ƒ" â†’ å˜æˆæ¦‚ç‡æƒé‡
RMSNorm: å½’ä¸€åŒ–"å‘é‡å¤§å°" â†’ ç¨³å®šè®­ç»ƒ

å®Œå…¨ä¸åŒçš„å½’ä¸€åŒ–ï¼
ä½ç½®ä¸åŒï¼Œç”¨é€”ä¸åŒï¼Œå…¬å¼ä¸åŒï¼
```

---

## ä¸ƒã€RoPE åœ¨ Attention ä¸­çš„åº”ç”¨

### 7.1 åº”ç”¨ä½ç½®

RoPE åœ¨ç”Ÿæˆ Qã€K ä¹‹åï¼Œè®¡ç®— Attention ä¹‹å‰æ–½åŠ ï¼š

```python
def forward(self, x, position_embeddings):
    # 1. ç”Ÿæˆ Qã€Kã€V
    Q = self.q_proj(x)
    K = self.k_proj(x)
    V = self.v_proj(x)

    # 2. æ‹†åˆ†å¤šå¤´
    Q = Q.view(batch, seq_len, num_heads, head_dim)
    K = K.view(batch, seq_len, num_heads, head_dim)
    V = V.view(batch, seq_len, num_heads, head_dim)

    # 3. è½¬ç½®
    Q = Q.transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]
    K = K.transpose(1, 2)

    # 4. â­ åº”ç”¨ RoPEï¼ˆåªå¯¹ Qã€Kï¼‰
    cos, sin = position_embeddings
    Q, K = apply_rotary_pos_emb(Q, K, cos, sin)

    # 5. è®¡ç®— Attention
    scores = Q @ K.transpose(-2, -1) / sqrt(head_dim)
    attn = softmax(scores, dim=-1)
    output = attn @ V

    return output
```

### 7.2 ä¸ºä»€ä¹ˆåªæ—‹è½¬ Q å’Œ Kï¼Ÿ

å›é¡¾å‰é¢æåˆ°çš„å†…å®¹ï¼š

**åŸå› **ï¼š
- Q å’Œ K è®¡ç®—**ç›¸ä¼¼åº¦** â†’ éœ€è¦ä½ç½®ä¿¡æ¯
- V è¡¨ç¤º**å†…å®¹** â†’ ä¸éœ€è¦ä½ç½®ä¿¡æ¯

**æµç¨‹**ï¼š
```
1. scores = Q @ K.T  â† è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆéœ€è¦ä½ç½®ï¼‰
2. weights = softmax(scores)
3. output = weights @ V  â† åŠ æƒæ±‚å’Œå†…å®¹ï¼ˆä¸éœ€è¦ä½ç½®ï¼‰
```

**ç±»æ¯”**ï¼š
- Qã€K æ˜¯"åœ°å›¾åæ ‡" â†’ éœ€è¦ RoPE
- V æ˜¯"å®è—å†…å®¹" â†’ ä¸éœ€è¦ RoPE

---


## å…«ã€åŠ¨æ‰‹å®éªŒ

å®Œæ•´çš„å­¦ä¹ ææ–™å·²å¼€æºï¼Œä½ å¯ä»¥è‡ªå·±è¿è¡ŒéªŒè¯ï¼š

```bash
# å…‹éš†ä»£ç 
git clone https://github.com/joyehuang/minimind-notes
cd minimind-notes/learning_materials

# å®éªŒ1ï¼šQã€Kã€V åŸºç¡€åŸç†
python attention_qkv_explained.py

# å®éªŒ2ï¼šMulti-Head Attention å®ç°
python multihead_attention.py

# å®éªŒ3ï¼šSoftmax vs RMSNorm å¯¹æ¯”
python softmax_vs_rmsnorm.py
```

---

## ä¹ã€æ€»ç»“

### 9.1 æ ¸å¿ƒè¦ç‚¹

- âœ… **Attention çš„æœ¬è´¨**ï¼šè®¡ç®—è¯ä¸è¯çš„ç›¸å…³æ€§ï¼Œèåˆä¸Šä¸‹æ–‡
- âœ… **Qã€Kã€V**ï¼šæ•°æ®åº“æŸ¥è¯¢ç±»æ¯”ï¼Œä¸æ˜¯ç„å­¦
- âœ… **æƒé‡çŸ©é˜µ**ï¼šè®­ç»ƒå­¦ä¹ çš„å‚æ•°ï¼Œå­˜åœ¨æ¨¡å‹æ–‡ä»¶é‡Œ
- âœ… **4 æ­¥æµç¨‹**ï¼šç›¸ä¼¼åº¦ â†’ ç¼©æ”¾ â†’ Softmax â†’ åŠ æƒæ±‚å’Œ
- âœ… **Multi-Head**ï¼š8 å‰¯çœ¼é•œçœ‹åŒä¸€å¥è¯ï¼Œèåˆå¤šä¸ªè§†è§’
- âœ… **Softmax â‰  RMSNorm**ï¼šå®Œå…¨ä¸åŒçš„å½’ä¸€åŒ–ï¼Œä½ç½®å’Œç”¨é€”éƒ½ä¸åŒ
- âœ… **RoPE åªç”¨äº Qã€K**ï¼šç›¸ä¼¼åº¦éœ€è¦ä½ç½®ï¼Œå†…å®¹ä¸éœ€è¦

### 9.2 Attention çš„ 4 æ­¥æµç¨‹ï¼ˆè®°å¿†ï¼‰

```
1. Q @ K.T          â†’ è®¡ç®—ç›¸ä¼¼åº¦
2. / âˆšd             â†’ ç¼©æ”¾
3. softmax(...)     â†’ å½’ä¸€åŒ–æˆæ¦‚ç‡
4. @ V              â†’ åŠ æƒæ±‚å’Œ
```

### 9.3 Multi-Head çš„ç»´åº¦å˜åŒ–ï¼ˆè®°å¿†ï¼‰

```
[batch, seq, 768]
  â†’ ç”Ÿæˆ Qã€Kã€V
  â†’ æ‹†åˆ†æˆ 8 å¤´: [batch, seq, 8, 96]
  â†’ è½¬ç½®: [batch, 8, seq, 96]
  â†’ Attentionï¼ˆå¹¶è¡Œï¼‰
  â†’ è½¬å›: [batch, seq, 8, 96]
  â†’ åˆå¹¶: [batch, seq, 768]
```

### 9.4 å…³é”®ä»£ç ä½ç½®ï¼ˆMiniMindï¼‰

- Attention å®ç°ï¼š`model/model_minimind.py:140-220`
- Qã€Kã€V æŠ•å½±ï¼š`model/model_minimind.py:159-161`
- RoPE åº”ç”¨ï¼š`model/model_minimind.py:182`
- å­¦ä¹ ç¤ºä¾‹ï¼š`learning_materials/attention_qkv_explained.py`

### 9.5 å»¶ä¼¸æ€è€ƒ

1. **GQA (Grouped Query Attention)**ï¼š
   - MiniMind ä½¿ç”¨ GQAï¼ˆ`num_key_value_heads=2`ï¼‰
   - èŠ‚çœæ˜¾å­˜ï¼ŒåŠ å¿«æ¨ç†

2. **Flash Attention**ï¼š
   - ä¼˜åŒ– Attention çš„è®¡ç®—å’Œæ˜¾å­˜è®¿é—®
   - è®­ç»ƒé€Ÿåº¦æå‡ 2-3 å€

3. **Sparse Attention**ï¼š
   - ä¸æ˜¯æ‰€æœ‰è¯éƒ½è¦å…³æ³¨æ‰€æœ‰è¯
   - é•¿æ–‡æœ¬åœºæ™¯çš„ä¼˜åŒ–

---

## åã€å‚è€ƒèµ„æ–™

**è®ºæ–‡**ï¼š
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer åŸå§‹è®ºæ–‡
- [GQA: Training Generalized Multi-Query Transformer](https://arxiv.org/abs/2305.13245) - GQA è®ºæ–‡
- [FlashAttention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135)

**ä»£ç **ï¼š
- MiniMind æºç ï¼š[github.com/jingyaogong/minimind](https://github.com/jingyaogong/minimind)
- Attention å®ç°ï¼š`model/model_minimind.py:140-220`

**ç³»åˆ—å…¶ä»–æ–‡ç« **ï¼š
- [ç¬¬1ç¯‡ï¼šå½’ä¸€åŒ–æœºåˆ¶](../20251216%20-%20normalization/post.mdx)
- [ç¬¬2ç¯‡ï¼šRoPEä½ç½®ç¼–ç ](../20251217%20-%20rope-position-encoding/post.mdx)
- [ç¬¬4ç¯‡ï¼šFeedForwardä¸å®Œæ•´æ¶æ„](../20251219%20-%20feedforward-transformer-block/post.mdx)

---

**æœ¬æ–‡ä½œè€…**ï¼šjoye
**å‘å¸ƒæ—¥æœŸ**ï¼š2025-12-29
**æœ€åæ›´æ–°**ï¼š2025-12-29
**ç³»åˆ—æ–‡ç« **ï¼šMiniMind å­¦ä¹ ç¬”è®°ï¼ˆ3/4ï¼‰

å¦‚æœè§‰å¾—æœ‰å¸®åŠ©ï¼Œæ¬¢è¿ï¼š
- â­ Star åŸé¡¹ç›® [MiniMind](https://github.com/jingyaogong/minimind)
- â­ Star æˆ‘çš„å­¦ä¹ ç¬”è®° [minimind-notes](https://github.com/joyehuang/minimind-notes)
- ğŸ’¬ ç•™è¨€è®¨è®ºä½ çš„å­¦ä¹ å¿ƒå¾—
- ğŸ”— åˆ†äº«ç»™å…¶ä»–å­¦ä¹  LLM çš„æœ‹å‹
